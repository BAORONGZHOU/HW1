\documentclass[11pt]{article}
\usepackage{common}
\title{CS 287 \\ Assignment 1: Text Classification }
\date{}
\begin{document}

\maketitle{}

In this homework assignment you will replicate current
research baseline results for text classification. Before 
starting the assignment be sure that you understand the model 
describe in the slides, in the text, and that you are familiar 
with the computational tutorial in HDF5 and Torch. 


You will be given a set of example sentences and labels. For instance: 


\vspace{0.5cm}

\noindent
\texttt{stupid , infantile , redundant , sloppy , over-the-top , and amateurish . yep , it's " waking up in reno . " go back to sleep . }

You results will be evaluated based on the Kaggle competition dataset at \url{}.

\section{Preprocessing}

For this assignment we have provided a sample set of preprocessing
code in \texttt{preprocessing.py}. Please read the code carefully, as
you will be expected to write this preprocessing in future
assignments.  The code reads in text examples and writes them out as
matrices containing the sparse features for each instance and the
class. Our basic code just outputs unigram features from the data.
The file will output a HDF5 bundle containing the following matrices:
\texttt{train\_input, train\_output, valid\_input, valid\_output, test\_input, nfeatures, nclasses}.

\textbf{Note} the structure of \texttt{X\_input} and
\texttt{X\_output}. The input consists of features, where a single
contains the feature indices $f_1, \ldots, f_k$. However because there
are differing number of features for each input the row is padded with
1's to $k_{max}$ (which should be ignored in your code).  For this
assignment, you should never need to explicitly construct the sparse
vector $\boldx$. The output consists of the correct class index for
each input. We do not provide the correct output for the test set.

\section{Classifier}

The code for your project will be in \texttt{HW1.lua} and should be
entirely written in Lua/Torch. For this assignment, you should not use
any non-debugging libraries besides the standard Torch framework and
HDF5.

\subsection{Prediction and Evaluation}

Be sure you are able to read and output the text data 
and that you understand the format. To confirm this, write a function 
that takes in the parameters of a linear model, and evaluates it on 
the validation set (\texttt{valid\_input} and \texttt{valid\_output}).
Report the results in terms of prediction accuracy. This same code should 
be used for each of the linear models described in this project. 

Also be sure you are able output your classification results on the test data as 
a text file. Check that you can upload these to the Kaggle.

\subsection{Hyperparameters}

Several of the models described have explicit hyperparameters that you will 
need to tune. It is your responsibility to cleanly separate these out from 
the models themselves and expose as command-line options. This makes it much 
easier to run experiments and to utilize experimental scripts. 

\subsection{Logging and Reporting}

As part of the write-up, you will need to report on the training and
predictive accuracy of your models. To make this possible, your code
should report on various metrics of the model both at training and
test time. We will leave it up to you on which metrics to log, but we
recommend reporting training speed, training set NLL, training set
predictive accuracy, and validation predictive accuracy. It is your
responsibility to convince us that the model is correctly training.

\section{Models}

For this assignment you will implement the three linear models that 
we discussed in class: naive bayes, multiclass logistic regression, 
and multiclass linear svm.

\subsection{Naive Bayes}

We recommend that you implement multinomial naive Bayes first, since
it is the most efficient and easiest to debug. Follow the description
given in the class notes and be sure to include the $\alpha$
hyperparameter.

\subsection{Logistic Regression}

Next implement multiclass logistic regression with L2 regularization
(exposing the $\lambda$ hyperparameter). For efficiency, training
should be done using minibatch stochastic gradient descent. The size
of the minibatch, the learning rate, and the number of epochs of
training should also be exposed as hyperparameters.

Note that there are several tutorials for how to implement logistic
regression in Torch online. Do not use these. We expect the
implementation to only utilize the core tensor operations in the
framework.

Further notes:

\begin{itemize}
\item It is important to use the log-sum-exp trick discussed in the
  class notes. Without this, you will likely run into numerical
  issues.

\item The Torch \texttt{index} and \texttt{indexAdd} operations should 
  be helpful for improving efficiency.

\item We recommend starting with a subset of the full training data to
  verify that your code is working. You can confirm this by tracking
  the loss of the training set.
\end{itemize}

\subsection{Linear SVM}

Finally implement hinge-loss with L2 regularization (exposing the $C$
hyperparameter). This code should use the same minibatch stochastic
gradient descent code as Logistic Regression. The only difference is
the calculation of the loss and the gradients. 

\subsection{Additional Experiments}

Once these models are constructed, you should also report on
additional experiments on these data sets. We will leave this aspect
open-ended, but suggestion include:

\begin{itemize}
\item Including additional features (for instance bigram features \cite{})
\item Experimenting with further models (for instance \cite{})
\item Extensive further hyperparameter tuning and experimentation (for instance \cite{})
\item Running on specific data sets. 
\end{itemize}

\section{Report and Submission}

For your write-up, follow the report template at
\url{https://github.com/cs287/hw_template}. Be sure to include a link
to your code, Kaggle ID, and reports on your results.

In addition to submitting your Kaggle results, we also expect you to report on your 
experimental process. This should include data tables, graphs and discussion of any 
issues that you may run into. 


\end{document}
